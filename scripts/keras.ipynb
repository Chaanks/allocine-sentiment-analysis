{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import numpy\n",
    "import json\n",
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "import string\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(name: str , input_file: Path, output_dir: Path):\n",
    "    \"\"\" extract json dataset to a specified path in .txt\"\"\"\n",
    "    \n",
    "    # create output dir\n",
    "    dataset_dir = output_dir / name\n",
    "    dataset_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Loading raw json file\n",
    "    with open(input_file, 'r', encoding='utf8') as file:   \n",
    "        raw_dataset = json.load(file)\n",
    "    \n",
    "    # write files\n",
    "    for idx, review in enumerate(tqdm(raw_dataset)):\n",
    "        label_dir = dataset_dir / str(review['note'])\n",
    "        label_dir.mkdir(parents=True, exist_ok=True)\n",
    "        filename = label_dir / f\"{review['review_id'].replace('review_', '')}.txt\"\n",
    "        filename.touch()\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(review['commentaire'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"../data/allocine\")\n",
    "train_json = Path(\"../data/json/train.json\")\n",
    "dev_json = Path(\"../data/json/dev.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100400/100400 [00:07<00:00, 13851.09it/s]\n",
      "100%|██████████| 665962/665962 [00:50<00:00, 13315.78it/s]\n"
     ]
    }
   ],
   "source": [
    "create_dataset('dev', dev_json, output_dir)\n",
    "create_dataset('train', train_json, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 665947 files belonging to 10 classes.\n",
      "Found 100399 files belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    \"../data/allocine_filtered/train\",\n",
    "    label_mode='categorical',\n",
    "    batch_size=batch_size\n",
    ")\n",
    "raw_dev_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    \"../data/allocine_filtered/eval\",\n",
    "    label_mode='categorical',\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in raw_train_ds: 20811\n",
      "Number of batches in raw_val_ds: 3138\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Number of batches in raw_train_ds: %d\"\n",
    "    % tf.data.experimental.cardinality(raw_train_ds)\n",
    ")\n",
    "print(\n",
    "    \"Number of batches in raw_val_ds: %d\" \n",
    "    % tf.data.experimental.cardinality(raw_dev_ds)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'J\\'ai pay\\xc3\\xa9 1 euro 50 ma place et c\\'est tout aussi bien. Je n\\'attendais vraiment rien de ce film, et je n\\'ai pas \\xc3\\xa9t\\xc3\\xa9 d\\xc3\\xa9\\xc3\\xa7u....ce genre d\\'humour qui, pour moi, ne peut faire rire que des gamins de 14/15 ans, une qualit\\xc3\\xa9 de jeu de la part des acteurs mauvaise, ou m\\xc3\\xaame proche du n\\xc3\\xa9ant par moments (trop fr\\xc3\\xa9quents malheureusement), des moments r\\xc3\\xa9ellement g\\xc3\\xaanants (toute derni\\xc3\\xa8re sc\\xc3\\xa8ne du film), des sc\\xc3\\xa8nes qui essaient d\\'\\xc3\\xa9mouvoir mais qui se retrouve \\xc3\\xa0 imiter les plus gros nanars de l\\'Histoire avec un grand H (prise de conscience path\\xc3\\xa9tique, discours nanardesques, etc...). Ce film se voit agr\\xc3\\xa9ment\\xc3\\xa9 de sc\\xc3\\xa8nes pr\\xc3\\xa9visibles (       spoiler:        On sait d\\xc3\\xa8s le d\\xc3\\xa9but du film que la vid\\xc3\\xa9o du mariage va \\xc3\\xaatre supprim\\xc3\\xa9e du t\\xc3\\xa9l\\xc3\\xa9phone     ). Le malaise se retrouve aussi dans le g\\xc3\\xa9n\\xc3\\xa9rique avec un \"b\\xc3\\xaatisier\" qui ne fait m\\xc3\\xaame pas esquisser un sourire au spectateur, et qui est, je suppose, compos\\xc3\\xa9 de sc\\xc3\\xa8nes tourn\\xc3\\xa9e POUR ce b\\xc3\\xaatisier uniquement) Enfin, il s\\'agit donc d\\'une com\\xc3\\xa9die am\\xc3\\xa9ricaine vide, plate, sans saveur. L\\xc3\\xa0 o\\xc3\\xb9 de tr\\xc3\\xa8s bons films comme Very Bad Trip arrivaient \\xc3\\xa0 inventer des blagues, des situations; Hors Contr\\xc3\\xb4le ne fait qu\\'encha\\xc3\\xaener des tentatives de gags non-stop, le tout en for\\xc3\\xa7ant le spectateur \\xc3\\xa0 supporter des prestations plus que m\\xc3\\xa9diocres de la part des acteurs. Un film qui se fera, et c\\'est pour le mieux, tr\\xc3\\xa8s vite oubli\\xc3\\xa9.'\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "b\"Une absence cruelle de rythme, tr\\xc3\\xa8s peu de gags qui peuvent nous faire rire, edouard aber totalement inepte en asterix: une v\\xc3\\xa9ritable d\\xc3\\xa9sillusion. Ou est l'ambiance des BD, le village, les sangliers, la ruse d'asterix. En outre laurent tirard qui s'est cru \\xc3\\xaatre le nouveau grand r\\xc3\\xa9alisateur fran\\xc3\\xa7ais de com\\xc3\\xa9dies d\\xc3\\xa9montre qu'il n'est qu'un pi\\xc3\\xa8tre technicien incapable d'imaginer quelques sc\\xc3\\xa8nes valables.\"\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "b\"Une belle le\\xc3\\xa7on de vie, un message d'espoir pour d\\xc3\\xa9buter cette nouvelle ann\\xc3\\xa9e ! Je ne comprends pas La note presse, je trouve ce film tr\\xc3\\xa8s touchant, tr\\xc3\\xa8s bien jou\\xc3\\xa9 et avec quelques touches d'humour. Tout ce qu'il faut pour en faire un bon film\"\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "b\"J'ai pas vu un film incroyable en comparaison de toute les bonnes critiques pour The Town. N\\xc3\\xa9anmoins \\xc3\\xa7a se regarde plut\\xc3\\xb4t bien.\"\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "b'Un film sympa mais avec une mise en sc\\xc3\\xa8ne tr\\xc3\\xa8s ordinaire qui ne convainc qu\\xe2\\x80\\x99\\xc3\\xa0 demi On ne s\\xe2\\x80\\x99ennuie pas mais il manque de profondeur.'\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in raw_train_ds.take(1):\n",
    "    for i in range(5):\n",
    "        print(text_batch.numpy()[i])\n",
    "        print(label_batch.numpy()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    tokens = tf.strings.lower(input_data)\n",
    "    tokens = tf.strings.regex_replace(tokens, '<br />', ' ')\n",
    "    tokens = tf.strings.regex_replace(tokens, 'http\\S+', '')\n",
    "    tokens = tf.strings.regex_replace(tokens, '[^\\d|(a-z)|!|#|@|è|é|à|ù|ô|ü|ë|ä|û|î|ê|â|ç\\s]', ' ')\n",
    "    tokens = tf.strings.regex_replace(tokens, '\\s+', ' ')\n",
    "    #tokens = tf.strings.regex_replace(tokens, '(.)\\1+', ' ')\n",
    "    #tokens = tf.strings.regex_replace(tokens, '[%s]' % re.escape(string.punctuation), '')\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model constants.\n",
    "max_features = 20000\n",
    "embedding_dim = 300\n",
    "sequence_length = 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer = TextVectorization(\n",
    "    standardize=None,\n",
    "    max_tokens=max_features,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ds = raw_train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label\n",
    "\n",
    "\n",
    "# Vectorize the data.\n",
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "dev_ds = raw_dev_ds.map(vectorize_text)\n",
    "\n",
    "# Do async prefetching / buffering of the data for best performance on GPU.\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
    "dev_ds = dev_ds.cache().prefetch(buffer_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "# A integer input for vocab indices.\n",
    "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "# Next, we add a layer to map those vocab indices into a space of dimensionality\n",
    "# 'embedding_dim'.\n",
    "x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Conv1D + global max pooling\n",
    "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "predictions = layers.Dense(10, activation=\"softmax\", name=\"predictions\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, predictions)\n",
    "\n",
    "# Compile the model with binary crossentropy loss and an adam optimizer.\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "20812/20812 [==============================] - 697s 33ms/step - loss: 1.5241 - accuracy: 0.3833 - val_loss: 1.4936 - val_accuracy: 0.3942\n",
      "Epoch 2/50\n",
      "20812/20812 [==============================] - 697s 33ms/step - loss: 1.5021 - accuracy: 0.3913 - val_loss: 1.4842 - val_accuracy: 0.4000\n",
      "Epoch 3/50\n",
      "20812/20812 [==============================] - 697s 33ms/step - loss: 1.4897 - accuracy: 0.3947 - val_loss: 1.4903 - val_accuracy: 0.3918\n",
      "Epoch 4/50\n",
      "20812/20812 [==============================] - 696s 33ms/step - loss: 1.4793 - accuracy: 0.3988 - val_loss: 1.4796 - val_accuracy: 0.4010\n",
      "Epoch 5/50\n",
      "20812/20812 [==============================] - 697s 33ms/step - loss: 1.4708 - accuracy: 0.4020 - val_loss: 1.4876 - val_accuracy: 0.4011\n",
      "Epoch 6/50\n",
      "20812/20812 [==============================] - 697s 33ms/step - loss: 1.4647 - accuracy: 0.4040 - val_loss: 1.4970 - val_accuracy: 0.3981\n",
      "Epoch 7/50\n",
      "20812/20812 [==============================] - 697s 33ms/step - loss: 1.4591 - accuracy: 0.4058 - val_loss: 1.4942 - val_accuracy: 0.3982\n",
      "Epoch 8/50\n",
      "20812/20812 [==============================] - 697s 33ms/step - loss: 1.4517 - accuracy: 0.4086 - val_loss: 1.5045 - val_accuracy: 0.3961\n",
      "Epoch 9/50\n",
      "20812/20812 [==============================] - 697s 33ms/step - loss: 1.4486 - accuracy: 0.4095 - val_loss: 1.5022 - val_accuracy: 0.3954\n",
      "Epoch 10/50\n",
      "20812/20812 [==============================] - 696s 33ms/step - loss: 1.4433 - accuracy: 0.4113 - val_loss: 1.5049 - val_accuracy: 0.3992\n",
      "Epoch 11/50\n",
      "20812/20812 [==============================] - 697s 33ms/step - loss: 1.4390 - accuracy: 0.4117 - val_loss: 1.5080 - val_accuracy: 0.3961\n",
      "Epoch 12/50\n",
      "20812/20812 [==============================] - 696s 33ms/step - loss: 1.4333 - accuracy: 0.4143 - val_loss: 1.5149 - val_accuracy: 0.3978\n",
      "Epoch 13/50\n",
      "20812/20812 [==============================] - 697s 33ms/step - loss: 1.4313 - accuracy: 0.4158 - val_loss: 1.5133 - val_accuracy: 0.3969\n",
      "Epoch 14/50\n",
      "20812/20812 [==============================] - 696s 33ms/step - loss: 1.4296 - accuracy: 0.4159 - val_loss: 1.5178 - val_accuracy: 0.3924\n",
      "Epoch 15/50\n",
      "20812/20812 [==============================] - 697s 33ms/step - loss: 1.4245 - accuracy: 0.4174 - val_loss: 1.5117 - val_accuracy: 0.3970\n",
      "Epoch 16/50\n",
      "20812/20812 [==============================] - 696s 33ms/step - loss: 1.4208 - accuracy: 0.4187 - val_loss: 1.5256 - val_accuracy: 0.3978\n",
      "Epoch 17/50\n",
      "20812/20812 [==============================] - 696s 33ms/step - loss: 1.4200 - accuracy: 0.4200 - val_loss: 1.5196 - val_accuracy: 0.3940\n",
      "Epoch 18/50\n",
      "20812/20812 [==============================] - 697s 33ms/step - loss: 1.4160 - accuracy: 0.4214 - val_loss: 1.5210 - val_accuracy: 0.3959\n",
      "Epoch 19/50\n",
      "20812/20812 [==============================] - 697s 33ms/step - loss: 1.4124 - accuracy: 0.4224 - val_loss: 1.5249 - val_accuracy: 0.3954\n",
      "Epoch 20/50\n",
      "20812/20812 [==============================] - 697s 33ms/step - loss: 1.4102 - accuracy: 0.4228 - val_loss: 1.5351 - val_accuracy: 0.3886\n",
      "Epoch 21/50\n",
      "20812/20812 [==============================] - 697s 33ms/step - loss: 1.4069 - accuracy: 0.4237 - val_loss: 1.5340 - val_accuracy: 0.3923\n",
      "Epoch 22/50\n",
      "20812/20812 [==============================] - 696s 33ms/step - loss: 1.4049 - accuracy: 0.4250 - val_loss: 1.5333 - val_accuracy: 0.3959\n",
      "Epoch 23/50\n",
      "20812/20812 [==============================] - 697s 33ms/step - loss: 1.4014 - accuracy: 0.4265 - val_loss: 1.5411 - val_accuracy: 0.3886\n",
      "Epoch 24/50\n",
      "20812/20812 [==============================] - 697s 33ms/step - loss: 1.3995 - accuracy: 0.4271 - val_loss: 1.5400 - val_accuracy: 0.3922\n",
      "Epoch 25/50\n",
      "20812/20812 [==============================] - 697s 33ms/step - loss: 1.3958 - accuracy: 0.4284 - val_loss: 1.5532 - val_accuracy: 0.3923\n",
      "Epoch 26/50\n",
      "20812/20812 [==============================] - 696s 33ms/step - loss: 1.3942 - accuracy: 0.4294 - val_loss: 1.5581 - val_accuracy: 0.3921\n",
      "Epoch 27/50\n",
      "20812/20812 [==============================] - 697s 33ms/step - loss: 1.3938 - accuracy: 0.4293 - val_loss: 1.5476 - val_accuracy: 0.3947\n",
      "Epoch 28/50\n",
      "20812/20812 [==============================] - 698s 34ms/step - loss: 1.3921 - accuracy: 0.4302 - val_loss: 1.5676 - val_accuracy: 0.3940\n",
      "Epoch 29/50\n",
      "20812/20812 [==============================] - 697s 33ms/step - loss: 1.3908 - accuracy: 0.4306 - val_loss: 1.5769 - val_accuracy: 0.3887\n",
      "Epoch 30/50\n",
      "20812/20812 [==============================] - 697s 33ms/step - loss: 1.3884 - accuracy: 0.4310 - val_loss: 1.5842 - val_accuracy: 0.3913\n",
      "Epoch 31/50\n",
      "20812/20812 [==============================] - 697s 33ms/step - loss: 1.3858 - accuracy: 0.4317 - val_loss: 1.5961 - val_accuracy: 0.3910\n",
      "Epoch 32/50\n",
      "20812/20812 [==============================] - 698s 34ms/step - loss: 1.3843 - accuracy: 0.4330 - val_loss: 1.5850 - val_accuracy: 0.3867\n",
      "Epoch 33/50\n",
      "20812/20812 [==============================] - 697s 33ms/step - loss: 1.3804 - accuracy: 0.4347 - val_loss: 1.5817 - val_accuracy: 0.3863\n",
      "Epoch 34/50\n",
      "20812/20812 [==============================] - 697s 33ms/step - loss: 1.3805 - accuracy: 0.4343 - val_loss: 1.5894 - val_accuracy: 0.3833\n",
      "Epoch 35/50\n",
      "20812/20812 [==============================] - 697s 33ms/step - loss: 1.3781 - accuracy: 0.4354 - val_loss: 1.5989 - val_accuracy: 0.3903\n",
      "Epoch 36/50\n",
      "20812/20812 [==============================] - 697s 33ms/step - loss: 1.3759 - accuracy: 0.4346 - val_loss: 1.6185 - val_accuracy: 0.3881\n",
      "Epoch 37/50\n",
      "20812/20812 [==============================] - 697s 33ms/step - loss: 1.3749 - accuracy: 0.4351 - val_loss: 1.6078 - val_accuracy: 0.3912\n",
      "Epoch 38/50\n",
      "20812/20812 [==============================] - 697s 33ms/step - loss: 1.3719 - accuracy: 0.4370 - val_loss: 1.6327 - val_accuracy: 0.3841\n",
      "Epoch 39/50\n",
      "20812/20812 [==============================] - 697s 33ms/step - loss: 1.3716 - accuracy: 0.4371 - val_loss: 1.6001 - val_accuracy: 0.3908\n",
      "Epoch 40/50\n",
      "20812/20812 [==============================] - 697s 33ms/step - loss: 1.3701 - accuracy: 0.4387 - val_loss: 1.6023 - val_accuracy: 0.3888\n",
      "Epoch 41/50\n",
      "20812/20812 [==============================] - 697s 33ms/step - loss: 1.3669 - accuracy: 0.4384 - val_loss: 1.6359 - val_accuracy: 0.3848\n",
      "Epoch 42/50\n",
      "20812/20812 [==============================] - 697s 33ms/step - loss: 1.3675 - accuracy: 0.4393 - val_loss: 1.6329 - val_accuracy: 0.3897\n",
      "Epoch 43/50\n",
      "20812/20812 [==============================] - 697s 33ms/step - loss: 1.3662 - accuracy: 0.4398 - val_loss: 1.5991 - val_accuracy: 0.3869\n",
      "Epoch 44/50\n",
      "14689/20812 [====================>.........] - ETA: 3:19 - loss: 1.3643 - accuracy: 0.4403"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-210-39a81a6c26c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Fit the model using the train and test datasets.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdev_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/innovation/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/innovation/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/innovation/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/innovation/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/innovation/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/envs/innovation/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/innovation/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "# Fit the model using the train and test datasets.\n",
    "model.fit(train_ds, validation_data=dev_ds, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/model_final/assets\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "model.save('../data/models/model_final')\n",
    "\n",
    "# load model\n",
    "# model = keras.models.load_model('path/to/location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the config and weights\n",
    "pickle.dump({'config': vectorize_layer.get_config(),\n",
    "             'weights': vectorize_layer.get_weights()}\n",
    "            , open(\"../data/models/model_final/tv_layer.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_disk = pickle.load(open(\"../data/models/model_final/tv_layer.pkl\", \"rb\"))\n",
    "new_v = TextVectorization.from_config(from_disk['config'])\n",
    "# You have to call `adapt` with some dummy data (BUG in Keras)\n",
    "#new_v.adapt(tf.data.Dataset.from_tensor_slices([\"xyz\"]))\n",
    "new_v.set_weights(from_disk['weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[15586     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]], shape=(1, 500), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[15586     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]], shape=(1, 500), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "text = 'é'\n",
    "text = tf.expand_dims(text, -1)\n",
    "#b = tf.strings.unicode_decode(x,'UTF-8')\n",
    "print(vectorize_layer(text))\n",
    "print(new_v(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We distribute pre-trained word vectors for 157 languages, trained on Common Crawl and Wikipedia using fastText. These models were trained using CBOW with position-weights, in dimension 300, with character n-grams of length 5, a window of size 5 and 10 negatives. We also distribute three new word analogy datasets, for French, Hindi and Polish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "path_to_glove_file = Path('../data/embedding/cc.fr.300.vec')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 18485 words (1515 misses)\n"
     ]
    }
   ],
   "source": [
    "voc = vectorize_layer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))\n",
    "num_tokens = len(voc) + 2\n",
    "embedding_dim = 300\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-531603d52f76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m embedding_layer = Embedding(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mnum_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0membeddings_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Embedding' is not defined"
     ]
    }
   ],
   "source": [
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
