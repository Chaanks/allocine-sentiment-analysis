{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import json\n",
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_comment(comment, nlp_model=None ) -> list:\n",
    "    \"\"\"\n",
    "    Tokenize a `comment` by removing hyperlinks, punctuation and extra spaces.\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    comment  : str\n",
    "        String to tokenize\n",
    "    nlp_model: optional (default: None)\n",
    "        NLP model to load for special tokenization (in particular stop words \n",
    "        removal during the tokenization filtering phase).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list(str)\n",
    "        The list of tokens extracted from the original comment\n",
    "    \"\"\"\n",
    "    re_punctuation = re.compile(r\"[^\\d|(a-z)|!||#|@|è|é|à|ù|ô|ü|ë|ä|û|î|ê|â|ç\\s]\")\n",
    "    re_hyperlink   = re.compile(r\"http\\S+\")\n",
    "    re_extra_space = re.compile(r\"\\s+\")\n",
    "    re_repetition  = re.compile(r'(.+?)\\1\\1+')\n",
    "    \n",
    "    tokens = re_hyperlink.sub(' ', comment.lower())\n",
    "    tokens = re_punctuation.sub(' ', tokens)\n",
    "    tokens = re_extra_space.sub(' ', tokens)\n",
    "    tokens = re_repetition.sub(r'\\1\\1\\1', tokens)\n",
    "    tokens = [ token for token in tokens.split() if len(token) > 1 ]\n",
    "\n",
    "    # Filtering out stop words from the `tmp` list\n",
    "    \n",
    "    if nlp_model is not None:\n",
    "        tokens = [ token for token in tokens if not token in nlp_model.Defaults.stop_words ]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = Path(\"../data/json/train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading raw json file\n",
    "with open(json_file, 'r', encoding='utf8') as file:   \n",
    "    raw_dataset = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and comments registration\n",
    "num_part   = 0\n",
    "num_elts   = len(raw_dataset)\n",
    "num_digits = len(f'{num_elts:_d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;37mLoading SpaCy en_core_web_sm model..\u001b[0m \u001b[0;34mDone!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Loading SpaCy model from tokenization utilities\n",
    "print(f'\\033[0;37mLoading SpaCy en_core_web_sm model..\\033[0m', end=' ')\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# Loading stop words\n",
    "stop_words_path = Path('/home/jarod/git/allocine-sentiment-analysis/data/json/stopwords.json')\n",
    "with open(stop_words_path, 'r', encoding='utf8') as file:   \n",
    "    stop_words = json.load(file)\n",
    "nlp.Defaults.stop_words = set(stop_words)\n",
    "print('\\033[0;34mDone!\\033[0m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;34mDone!\u001b[0m [ \u001b[0;37mnum_items: 665_962\u001b[0m ]\n"
     ]
    }
   ],
   "source": [
    "for idx, review in enumerate(raw_dataset):\n",
    "    review['lst_mots']  = tokenize_comment(review['commentaire'], nlp)\n",
    "    print(f'\\033[0;37mProgress: \\033[1;30m{idx:>{num_digits}_d}\\033[0m /{num_elts:_d}', end='\\r')\n",
    "    \n",
    "    if idx == 1500:\n",
    "        break\n",
    "    \n",
    "print(f'\\033[0;34mDone!\\033[0m [ \\033[0;37mnum_items: {num_elts:_d}\\033[0m ]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['helllo', '!!!']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_comment(\"hellllo !!!!\", nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy = df.head(1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = toy[toy['lst_mots'].apply(lambda x: len(x)>0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = toy[toy['lst_mots'].apply(lambda x: len(''.join(x))<2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = filtered.drop(columns=['name', 'user_id', 'commentaire', 'movie'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = filtered.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered.to_csv('../data/csv/train_1000.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = filtered.iloc[0]['lst_mots']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pastrouvefacetientcôtéphrasetaglineveutrésumeparfaitementpassciencefictionjosephkosinskiintéressepompierséliteprofessionnelshommesdressetrèsbeauportraitbandepotesprêtssacrifierprotégerenvironnementflammesdavantagecentréquotidienhérosfacedilemmesempêchepasscènesterrainsoientsuperbesterriblesvraimentimpressionnantestémoigneexcellentfinalhistoireinspiréefaitsréelsconnaissezpasmieuxtempsdécouvrirtraversexpériencebienforteefficacetrèsbonhistoirepuissantepleinehumanitétraitementsobretrèsjamaisforcétandisplantechniquetrèssolidescènesgrandréalismesommetrèsbeaudivertissement'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Argument 'string' has incorrect type (expected str, got list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-a65da5ce019b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Je mange du paain\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/innovation/lib/python3.8/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    439\u001b[0m                 \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             )\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcomponent_cfg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0mcomponent_cfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/innovation/lib/python3.8/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mmake_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_format_docs_and_golds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Argument 'string' has incorrect type (expected str, got list)"
     ]
    }
   ],
   "source": [
    "tokens = nlp(\"Je mange du paain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je True 46.5811 96\n",
      "mange True 41.34777 96\n",
      "du True 44.64348 96\n",
      "paain True 45.513462 96\n"
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    print(token.text, token.has_vector, token.vector_norm, len(token.vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
